{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluci√≥n y todas las suposiciones que est√°s considerando. Aqu√≠ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones\n",
    "\n",
    "En todos los casos abordados, se parte del supuesto de que los datos han pasado por una exhaustiva inspecci√≥n de calidad previa. Esto implica que se encuentran en un estado √≥ptimo, con todas las etiquetas correspondientes debidamente asignadas y sin ning√∫n tipo de corrupci√≥n.\n",
    "\n",
    "## Posibles mejoras y uso de GCP\n",
    "\n",
    "Considerando los requerimientos del proyecto, una mejora significativa podr√≠a ser la adopci√≥n de herramientas ofrecidas por Google Cloud Platform (GCP). Si bien se han propuesto soluciones utilizando BigTable para el almacenamiento de datos semiestructurados, como los tweets, resulta pertinente se√±alar que esta elecci√≥n podr√≠a no ser la m√°s √≥ptima.\n",
    "\n",
    "Por ejemplo, los tweets contienen una variedad de metadatos y relaciones que podr√≠an beneficiarse de un almacenamiento m√°s estructurado, como el que proporciona BigQuery. Por lo tanto, una mejora potencial ser√≠a aprovechar BigQuery para el almacenamiento de la informaci√≥n, ya que est√° especialmente dise√±ado para manejar grandes vol√∫menes de datos y permite consultas r√°pidas y flexibles sobre ellos.\n",
    "\n",
    "En cuanto al procesamiento de los datos, se podr√≠a seguir una l√≥gica escalonada para lograr una mayor eficiencia:\n",
    "\n",
    "1.  Definici√≥n de la consulta en BigQuery: Especificar la consulta necesaria para obtener los datos relevantes.\n",
    "2.  Ejecuci√≥n de la consulta desde Dataflow: Utilizar Dataflow para ejecutar la consulta definida en BigQuery y obtener los resultados de manera eficiente y escalable.\n",
    "3.  Procesamiento de los resultados: Aplicar transformaciones y operaciones de procesamiento necesarias, como la extracci√≥n de emojis, la identificaci√≥n de usuarios y fechas, sobre los datos obtenidos.\n",
    "4.  Escritura de resultados: Si el procesamiento es un proceso recurrente, almacenar los resultados en una plataforma de almacenamiento de datos adecuada, como Cloud Storage o BigQuery.\n",
    "\n",
    "Esta aproximaci√≥n, adem√°s de aprovechar la potencia y la escalabilidad de las herramientas de GCP, tambi√©n facilitar√≠a la integraci√≥n y el mantenimiento del sistema en el futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1_TIME\n",
    "\n",
    "La optimizaci√≥n del tiempo de ejecuci√≥n de q1_time se logr√≥ reduciendo significativamente el tiempo de procesamiento mediante la implementaci√≥n de t√©cnicas eficientes en el manejo de datos.\n",
    "\n",
    "Para mejorar la eficiencia, se adopt√≥ una estrategia m√°s sofisticada que aprovechaba el poder del multiprocesamiento. En esta versi√≥n optimizada, se realiz√≥ una lectura de los n√∫cleos disponibles para la ejecuci√≥n del c√≥digo, permitiendo as√≠ el procesamiento en paralelo. Esto signific√≥ dividir la lista de datos en lotes de igual tama√±o para ejecutar la funci√≥n de manera simult√°nea y reducir el tiempo total de ejecuci√≥n.\n",
    "\n",
    "La determinaci√≥n del tama√±o de estos lotes se realiz√≥ dividiendo la cantidad total de datos por la cantidad de n√∫cleos disponibles para la ejecuci√≥n. Si el tama√±o del lote era menor que la cantidad de n√∫cleos, se opt√≥ por utilizar un √∫nico lote para evitar el desperdicio de recursos.\n",
    "\n",
    "Posteriormente, utilizando funciones lambda, se identificaron las 10 fechas con la mayor cantidad de tweets. A partir de estas fechas, se procedi√≥ a determinar los usuarios con la mayor cantidad de tweets en cada una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "3.8702001571655273 segundos\n"
     ]
    }
   ],
   "source": [
    "from q1_time import q1_time\n",
    "import time\n",
    "\n",
    "tiempo_comienzo = time.time()\n",
    "print(q1_time(file_path))\n",
    "tiempo_final = time.time()\n",
    "print(str(tiempo_final-tiempo_comienzo) + ' segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 MEMORY\n",
    "\n",
    "La funci√≥n q1_memory ha sido dise√±ada para optimizar el uso de memoria al procesar grandes conjuntos de datos contenidos en el archivo JSON.\n",
    "\n",
    "Al abrir el archivo especificado en la ruta proporcionada, esta funci√≥n procesa cada l√≠nea del archivo de forma individual utilizando el paquete ujson. Esta estrategia permite minimizar el uso de memoria al cargar y procesar los datos de forma incremental, en lugar de cargar todo el archivo en memoria de una vez.\n",
    "\n",
    "Cada l√≠nea del archivo JSON se convierte en un objeto JSON utilizando la funci√≥n ujson.loads(). A continuaci√≥n, se extraen la fecha del tweet y el nombre de usuario asociado.\n",
    "\n",
    "Para almacenar y procesar la actividad por fecha y usuario, se utiliza un diccionario predeterminado de defaultdict(Counter). Este diccionario permite contar eficientemente la actividad de cada usuario en cada fecha, utilizando la estructura de datos Counter para realizar un seguimiento de los conteos.\n",
    "\n",
    "Una vez procesadas todas las l√≠neas del archivo y almacenada la actividad por fecha y usuario, se utiliza la funci√≥n heapq.nlargest() para encontrar las 10 fechas m√°s activas de manera eficiente.\n",
    "\n",
    "Para cada fecha seleccionada, se determina el usuario m√°s activo y se agrega al resultado final como una tupla que contiene la fecha y el usuario m√°s activo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "3.28125 Mb\n"
     ]
    }
   ],
   "source": [
    "from q1_memory import q1_memory\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "inicio_memoria = memory_usage()[0]\n",
    "print(q1_memory(file_path))\n",
    "fin_memoria = memory_usage()[0]\n",
    "print(str(fin_memoria-inicio_memoria) + ' Mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 TIME\n",
    "\n",
    "La funci√≥n q2_memory ha sido desarrollada con el objetivo de contar la frecuencia de emojis en los tweets presentes en un archivo JSON, optimizando el uso de memoria durante este proceso.\n",
    "\n",
    "Cada l√≠nea del archivo JSON se convierte en un objeto JSON utilizando la funci√≥n ujson.loads(). A continuaci√≥n, se extrae el contenido del tweet y se utilizan expresiones regulares Unicode para identificar y extraer los emojis presentes en el texto utilizando la funci√≥n extract_emojis. El uso de expresiones regulares Unicode garantiza una identificaci√≥n precisa de los emojis en el texto del tweet, lo que contribuye a la eficiencia general de la funci√≥n. Por otro lado, el uso de expresiones regulares arrojo el mejor resultado en temas de tiempos de procesamiento en relacion a otras librerias como emoji o emojis.\n",
    "\n",
    "Para almacenar y procesar la frecuencia de cada emoji, se utiliza un objeto Counter de la biblioteca est√°ndar de Python. Este objeto permite contar eficientemente la frecuencia de cada emoji encontrado en los tweets.\n",
    "\n",
    "Una vez procesadas todas las l√≠neas del archivo y contada la frecuencia de los emojis, se utiliza el m√©todo most_common() del objeto Counter para encontrar los 10 emojis m√°s utilizados de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 7286), ('üòÇ', 3072), ('üöú', 2972), ('‚úä', 2411), ('üåæ', 2363), ('üáÆ', 2096), ('üá≥', 2094), ('‚ù§', 1779), ('ü§£', 1668), ('üëá', 1108)]\n",
      "3.9355852603912354 segundos\n"
     ]
    }
   ],
   "source": [
    "from q2_time import q2_time\n",
    "import time\n",
    "\n",
    "tiempo_comienzo = time.time()\n",
    "print(q2_time(file_path))\n",
    "tiempo_final = time.time()\n",
    "print(str(tiempo_final-tiempo_comienzo) + ' segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2_MEMORY\n",
    "\n",
    "La funci√≥n q2_memory se encarga de analizar un archivo JSON que contiene tweets para contar y devolver los 10 emojis m√°s utilizados en el contenido de esos tweets.\n",
    "\n",
    "Al abrir el archivo especificado en la ruta proporcionada, la funci√≥n lee cada l√≠nea del archivo de manera secuencial utilizando la funci√≥n ujson.loads() para cargar cada tweet como un objeto JSON, lo cual lo hace mas efectivo en terminos de memoria. Luego, extrae el contenido del tweet y utiliza la funci√≥n extract_emojis() para obtener una lista de emojis presentes en ese contenido.\n",
    "\n",
    "Para llevar un seguimiento de la frecuencia de cada emoji, se utiliza un contador (Counter()) que se actualiza con los emojis extra√≠dos de cada tweet.\n",
    "\n",
    "Finalmente, la funci√≥n devuelve una lista de las 10 tuplas m√°s comunes, donde cada tupla contiene un emoji y su frecuencia de aparici√≥n en los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 7286), ('üòÇ', 3072), ('üöú', 2972), ('‚úä', 2411), ('üåæ', 2363), ('üèª', 2080), ('‚ù§', 1779), ('ü§£', 1668), ('üèΩ', 1218), ('üëá', 1108)]\n",
      "-1.23828125 Mb\n"
     ]
    }
   ],
   "source": [
    "from q2_memory    import q2_memory\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "inicio_memoria = memory_usage()[0]\n",
    "print(q2_memory(file_path))\n",
    "fin_memoria = memory_usage()[0]\n",
    "print(str(fin_memoria-inicio_memoria) + ' Mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3_TIME\n",
    "\n",
    "La funci√≥n q3_time se encarga de procesar un archivo de texto para encontrar los usuarios m√°s influyentes basados en menciones. Utiliza expresiones regulares para buscar y extraer las menciones de usuario en cada l√≠nea del archivo.\n",
    "\n",
    "Al abrir el archivo especificado en la ruta proporcionada, la funci√≥n itera sobre cada l√≠nea y busca todas las menciones de usuario utilizando la expresi√≥n regular @(\\w+). Cada vez que se encuentra una menci√≥n, se incrementa el contador de menciones para ese usuario en un diccionario user_mentions, que almacena el recuento de menciones de cada usuario.\n",
    "\n",
    "Una vez procesadas todas las l√≠neas del archivo y recopiladas las menciones de usuario, se ordenan los usuarios por la cantidad de menciones en orden descendente utilizando la funci√≥n sorted(). Luego, se devuelven las 10 tuplas m√°s mencionadas, donde cada tupla contiene el nombre de usuario y la cantidad de menciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 5300), ('Kisanektamorcha', 4090), ('RakeshTikaitBKU', 3728), ('PMOIndia', 3114), ('RahulGandhi', 2704), ('GretaThunberg', 2622), ('DelhiPolice', 2254), ('rihanna', 2246), ('RaviSinghKA', 2242), ('INCIndia', 2236)]\n",
      "1.2904407978057861 segundos\n"
     ]
    }
   ],
   "source": [
    "from q3_time import q3_time\n",
    "import time\n",
    "\n",
    "tiempo_comienzo = time.time()\n",
    "print(q3_time(file_path))\n",
    "tiempo_final = time.time()\n",
    "print(str(tiempo_final-tiempo_comienzo) + ' segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3_MEMORY\n",
    "\n",
    "La funci√≥n q3_memory est√° dise√±ada para analizar un archivo de texto y encontrar los usuarios m√°s influyentes basados en menciones. A diferencia de la funci√≥n q3_time, esta versi√≥n optimiza el uso de memoria al procesar el archivo de una manera m√°s eficiente.\n",
    "\n",
    "Para lograr esto, la funci√≥n utiliza un enfoque diferente para contar las menciones de usuario:\n",
    "\n",
    "Conjunto de Usuarios √önicos: Se inicializa un conjunto unique_users para almacenar usuarios √∫nicos con menciones. Este enfoque elimina la necesidad de almacenar cada menci√≥n individualmente, reduciendo as√≠ el uso de memoria al evitar duplicados.\n",
    "\n",
    "Diccionario de Conteo de Menciones: Se utiliza un diccionario user_mentions para almacenar los recuentos de menciones de usuario. Cada vez que se encuentra una menci√≥n en el archivo, se incrementa el contador correspondiente en el diccionario.\n",
    "\n",
    "Una vez procesado todo el archivo y recopiladas las menciones de usuario, se ordenan los usuarios por la cantidad de menciones en orden descendente utilizando la funci√≥n sorted(). Luego, se devuelven las 10 tuplas m√°s mencionadas, donde cada tupla contiene el nombre de usuario y la cantidad de menciones.\n",
    "\n",
    "Este enfoque optimizado minimiza el uso de memoria al procesar el archivo de texto de manera m√°s eficiente, lo que permite manejar archivos de gran tama√±o sin agotar los recursos de memoria disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 3364), ('PMOIndia', 2088), ('Kisanektamorcha', 1966), ('RakeshTikaitBKU', 1918), ('GretaThunberg', 1882), ('punyaab\",', 1722), ('DelhiPolice', 1638), ('jazzyb', 1522), ('RahulGandhi', 1498), ('rihanna', 1496)]\n",
      "0.51171875 Mb\n"
     ]
    }
   ],
   "source": [
    "from q3_memory    import q3_memory\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "inicio_memoria = memory_usage()[0]\n",
    "print(q3_memory(file_path))\n",
    "fin_memoria = memory_usage()[0]\n",
    "print(str(fin_memoria-inicio_memoria) + ' Mb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
